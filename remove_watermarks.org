#+TITLE: Remove Watermarks
#+AUTHOR: AntÃ³nio Mendes
#+DATE: 10 / 12 / 20
#+email: 17amendes@gmail.com
#+PROPERTY: header-args :exports both :session _rem_mrk :results value
#+OPTIONS: toc:nil num:nil
#+LaTeX_HEADER: \usepackage[left=3.5cm, top=2.5cm, right=3.5cm, includefoot]{geometry}
-----

* Imports
** standard
#+BEGIN_SRC python
import numpy  as np
import pandas as pd
import cv2
#+END_SRC

#+RESULTS:

** import os and getting current directory
#+BEGIN_SRC python :results file
import os 
from pathlib import Path
cwd = os.path.abspath(os.getcwd())
cwd
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project]]

*** =data_dir=
#+BEGIN_SRC python :results file
data_dir = Path("/".join(cwd.split("/"))) / "data"
if data_dir.exists() == False:
  os.mkdir(data_dir)

data_dir
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/data]]

**** =train_dir=
#+BEGIN_SRC python :results file
train_dir = data_dir / "train_set"

train_dir
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/data/train_set]]

**** =test_dir=
#+BEGIN_SRC python :results file
test_dir = data_dir / "test_set"
test_dir
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/data/test_set]]

**** =pexel_img_dir=
#+BEGIN_SRC python :results file
pexel_img_dir = data_dir / "pexel_img_dir" 
if pexel_img_dir.exists() == False:
  os.mkdir(pexel_img_dir)

pexel_img_dir
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/data/pexel_img_dir]]

**** =watermark_pexel_img_dir=
#+BEGIN_SRC python :results file
watermark_pexel_img_dir = data_dir / "watermark_pexel_img_dir"
if watermark_pexel_img_dir.exists() == False:
  os.mkdir(watermark_pexel_img_dir)

watermark_pexel_img_dir
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/data/watermark_pexel_img_dir]]

*** =model_dir=
#+BEGIN_SRC python :results file
model_dir = Path(cwd) / "models"
if model_dir.exists() == False:
  os.mkdir(model_dir)

model_dir
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/models]]

** tools
#+BEGIN_SRC python
from tqdm import tqdm
from PIL import Image
#+END_SRC

#+RESULTS:

** tensorflow
#+BEGIN_SRC python
import tensorflow as tf

from tensorflow.keras import layers
from keras.models import Sequential, Model, load_model
#+END_SRC

#+RESULTS:

** keras
#+BEGIN_SRC python
from keras.datasets.cifar10 import load_data
#+END_SRC

#+RESULTS:

** fastai
#+BEGIN_SRC python
import fastai


from fastai.vision import *
#+END_SRC

#+RESULTS:

* Data
** Convert Images into Tensors
#+BEGIN_SRC python
original_images = os.listdir(train_dir)
if ".DS_Store" in original_images:
  original_images.remove(".DS_Store")

tensor_images = []
for img_name in tqdm(original_images):
  img_rgb = cv2.cvtColor(cv2.imread(filename=str(train_dir / img_name)), cv2.COLOR_BGR2RGB)
  tensor_images.append(tf.convert_to_tensor(img_rgb, dtype=tf.float32))


tensor_images = np.asarray(tensor_images)
tensor_images.shape
#+END_SRC

#+RESULTS:
: /Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/data/train_set

#+BEGIN_SRC python :results file
np.save(data_dir / "train_tensors", tensor_images)
data_dir / "train_tensors"
#+END_SRC

** Normalise Tensors
#+BEGIN_SRC python
def normalize(image):
  image = tf.cast(image, tf.float32)
  image = (image / 127.5) - 1
  return image
#+END_SRC

#+RESULTS:

22717

#+BEGIN_SRC python
normalised_tensors = []
for tensor_img in tqdm(tensor_images):
  normalised_tensors.append(normalize(tensor_img))

normalised_tensors = np.asarray(normalised_tensors)
normalised_tensors.shape
#+END_SRC

#+RESULTS:

* Tranlsate Images
#+BEGIN_SRC python
remove_watermarks = tf.keras.models.load_model(model_dir / "generator_f")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
def crop_image_height(image):
  if image.shape[1] > 256:
    max_height = image.shape[1]
    max_slice = max_height - 257
    start_slice = np.random.randint(0, max_slice) if max_slice > 0 else 0
    end_slice = start_slice + 256
    return image[:,start_slice:end_slice,:]
  else: 
    return image
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
def crop_image_width(image):
  if image.shape[0] > 256:
    max_width = image.shape[0]
    max_slice = max_width - 257
    start_slice = np.random.randint(0, max_slice) if max_slice > 0 else 0
    end_slice = start_slice + 256
    return image[start_slice:end_slice,:,:]
  else: 
    return image
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
count = 0
mark_removed_tensors = []

for t_img in tqdm(normalised_tensors):
#for t_img in tqdm(tensor_images):
  cropped_img = crop_image_height(t_img)
  cropped_img = crop_image_width(cropped_img)
  mark_removed_tensors.append(remove_watermarks.predict(cropped_img[np.newaxis,:]))
  count += 1
  if count == 10000:
    break
#+END_SRC

#+RESULTS:

* Save Translation
#+BEGIN_SRC python
filenames = os.listdir(train_dir)
if ".DS_Store" in filenames:
  filenames.remove(".DS_Store")

no_of_files = len(filenames)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :results file
train_mrk_removed_dir = data_dir / "train_set_watermark_removed"
if train_mrk_removed_dir.exists() == False:
  os.mkdir(train_mrk_removed_dir)

data_dir
#+END_SRC

#+RESULTS:
[[file:/Users/antoniomendes/Desktop/uva_data_science/semester_01/block_02/AML/Kaggle_Project/data]]

#+BEGIN_SRC python
with tf.compat.v1.Session() as sess:
    init = tf.group(tf.compat.v1.global_variables_initializer(),tf.compat.v1.local_variables_initializer())
    sess.run(init)
    tf.compat.v1.train.start_queue_runners()
    for i in tqdm(range(no_of_files)):
        img_tnsr = mark_removed_tensors[i]
        img = sess.run(img_tnsr)
        img = Image.fromarray(img)
        img.save(train_mrk_removed_dir / filenames[i])
#+END_SRC

#+RESULTS:
: []

#+BEGIN_SRC python
for i in tqdm(range(no_of_files)):
  img_tnsr = mark_removed_tensors[i]
  img = sess.run(img_tnsr)
  img = Image.fromarray(img)
  img.save(train_mrk_removed_dir / filenames[i])
#+END_SRC

#+RESULTS:

* Test Set
** Data
#+BEGIN_SRC python
original_images = os.listdir(train_dir)
if ".DS_Store" in original_images:
  original_images.remove(".DS_Store")

tensor_images = []
for img_name in tqdm(original_images):
  img_rgb = cv2.cvtColor(cv2.imread(filename=str(train_dir / img_name)), cv2.COLOR_BGR2RGB)
  tensor_images.append(tf.convert_to_tensor(img_rgb, dtype=tf.float32))


tensor_images = np.asarray(tensor_images)
tensor_images.shape
#+END_SRC

#+BEGIN_SRC python
normalised_tensors = []
for tensor_img in tqdm(tensor_images):
  normalised_tensors.append(normalize(tensor_img))

normalised_tensors = np.asarray(normalised_tensors)
normalised_tensors.shape
#+END_SRC

** Translate Images
#+BEGIN_SRC python
remove_watermarks = tf.keras.models.load_model(model_dir / "generator_f")
#+END_SRC

#+BEGIN_SRC python
mark_removed_tensors = []

for t_img in tqdm(normalised_tensors):
  cropped_img = crop_image_height(t_img)
  cropped_img = crop_image_width(cropped_img)
  mark_removed_tensors.append(remove_watermarks.predict(cropped_img[np.newaxis,:]))
#+END_SRC

** Save Translation
#+BEGIN_SRC python
filenames = os.listdir(test_dir)
if ".DS_Store" in filenames:
  filenames.remove(".DS_Store")
no_of_files = len(filenames)
#+END_SRC

#+BEGIN_SRC python
test_mrk_removed_dir = data_dir / "test_set_watermark_removed"
if test_mrk_removed_dir.exists() == False:
  os.mkdir(test_mrk_removed_dir)

data_dir
#+END_SRC

#+BEGIN_SRC python
with tf.Session() as sess:
    init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())
    sess.run(init)
    tf.train.start_queue_runners()
    for i in tqdm(range(no_of_files)):
        img_tnsr = mark_removed_tensors[i]
        img = sess.run(img_tnsr)
        img = Image.fromarray(img)
        img.save(test_mrk_removed_dir / filenames[i])
#+END_SRC
