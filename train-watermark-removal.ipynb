{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Introduction](#introduction)\n",
    "* [Imports](#imports)\n",
    "    - [Standard](#standard)\n",
    "    - [Matplotlib](#matplotlib)\n",
    "    - [Import OS and Get/Make Directories](#import-os)\n",
    "    - [Tensorflow & Keras](#tensorflow)\n",
    "    - [Other Tools](#other-tools)\n",
    "* [Generate Data](#generate-data)\n",
    "    - [Load Pexel Images](#load-pexel-images)\n",
    "        -[search_terms](#search-terms)\n",
    "        -[Scrape Image from Online](#scrape-images)\n",
    "    - [Add Watermarks to Images](#add-watermarks)\n",
    "* [Make cycleGAN Model](#make-cycle-gan)\n",
    "    - [Data](#data)\n",
    "    - [Models](#models)\n",
    "    - [Loss Functions](#loss-functions)\n",
    "    - [Optimisers](#optimisers)\n",
    "    - [Checkpoints](#checkpoints)\n",
    "    - [Training](#training)\n",
    "    - [Save Models](#save-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  <a id=\"introduction\"></a>\n",
    "\n",
    "Before training, the ensemble model is optimised by removing watermarks from images in both the train and test sets. To do this, a cycleGAN is developed, which is a combination of two GANs: hence there are two \"generators\" and two \"discriminators\".  The first generator, G_w, makes watermarked images from normal images. The second generator model, G_n, generates normal images from watermarked images. After training the cycleGAN, this generator model, G_n, is used to remove watermarks. Not only does this notebook develop the cycleGAN, but it also scrapes images from online for training the cycleGAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports <a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard <a id=\"standard\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib <a id=\"matplotlib\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import OS and Get/Make Directories <a id=\"import-os\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = Path(\"/\".join(cwd.split(\"/\"))) / \"figures\"\n",
    "if fig_dir.exists() == False:\n",
    "    os.mkdir(fig_dir)\n",
    "\n",
    "fig_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/\".join(cwd.split(\"/\"))) / \"data\"\n",
    "if data_dir.exists() == False:\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pexel_img_dir = data_dir / \"pexel_img_dir\" \n",
    "if pexel_img_dir.exists() == False:\n",
    "    os.mkdir(pexel_img_dir)\n",
    "\n",
    "pexel_img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_pexel_img_dir = data_dir / \"watermark_pexel_img_dir\"\n",
    "if watermark_pexel_img_dir.exists() == False:\n",
    "    os.mkdir(watermark_pexel_img_dir)\n",
    "\n",
    "watermark_pexel_img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_dir = data_dir / \"fonts\"\n",
    "if font_dir.exists() == False:\n",
    "    os.mkdir(font_dir)\n",
    "\n",
    "font_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(cwd) / \"models\"\n",
    "if model_dir.exists() == False:\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow & Keras <a id=\"tensorflow\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow_examples.models.pix2pix import pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Tools <a id=\"other-tools\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import shutil\n",
    "import string\n",
    "import time\n",
    "import cv2 \n",
    "from pexels_api import API \n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data <a id=\"generate-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pexel Images <a id=\"load-pexel-images\"></a>\n",
    "\n",
    "The first step towards training any model is to collect the training data. For this cycleGAN, stock-free images are scraped from Pexels with an [API Key](https://www.pexels.com/api/new/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search_terms <a id=\"search-terms\"></a>\n",
    "\n",
    "Below is the list of search terms used to search for stock-free images. While most search terms are related to food items because of the main objective of the Kaggle Project, some few terms are unrelated, like \"kitten\" and \"people\", to confuse and challenge the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = [\n",
    "  \"kitten\",\n",
    "  \"dog\",\n",
    "  \"food\",\n",
    "  \"cake\",\n",
    "  \"pasta\",\n",
    "  \"people\",\n",
    "  \"steak\",\n",
    "  \"cooked chicken\",\n",
    "  \"chicken wings\",\n",
    "  \"crab food\",\n",
    "  \"seafood\",\n",
    "  \"oyster\",\n",
    "  \"donut\",\n",
    "  \"burger\",\n",
    "  \"pizza\",\n",
    "  \"egg\",\n",
    "  \"avocado\",\n",
    "  \"bread\",\n",
    "  \"salad\",\n",
    "  \"sandwich\",\n",
    "  \"fries\",\n",
    "  \"butter\",\n",
    "  \"ham\",\n",
    "  \"sausage\",\n",
    "  \"bacon\",\n",
    "  \"dessert\",\n",
    "  \"rice\",\n",
    "  \"lasagna\",\n",
    "  \"green peas\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Images from Online <a id=\"scrape-images\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pexel_folder_dir = pexel_img_dir / \"pexel_images\"\n",
    "if pexel_folder_dir.exists() == False:\n",
    "    os.mkdir(pexel_folder_dir)\n",
    "\n",
    "PEXELS_API_KEY = \"563492ad6f91700001000001f8f1cdfad4e84affa3d8bb8ea5312020\"\n",
    "\n",
    "api = API(PEXELS_API_KEY)\n",
    "\n",
    "for i, search_term in enumerate(tqdm(search_terms)):\n",
    "    print(\"search_term: \", search_term)\n",
    "    search = api.search(search_term, page=1, results_per_page=40)\n",
    "    photos = api.get_entries()\n",
    "    for j,photo in enumerate(photos):\n",
    "        img_url = photo.medium\n",
    "        filename = str(search_term + \"_\" + str(j) + \".jpg\")\n",
    "        r = requests.get(img_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(pexel_folder_dir / filename, 'wb') as f:\n",
    "                r.raw.decode_content = True\n",
    "                shutil.copyfileobj(r.raw, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Watermarks to Images <a id=\"add-watermarks\"></a>\n",
    "\n",
    "The next set in making the dataset for training is copy the collection of pexel images. With this copy, each image is watermarked. The result is that for every unmarked image, there is also an duplicate with watermarks, and vice-versa. While a cycleGAN does not need pairing a of images to train, it useful to still have this transition to stay organised and evaluate the performance of the cycleGAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the code in the block below is implemented from this [blog](https://rickwierenga.com/blog/machine%20learning/GanWatermark.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = os.listdir(font_dir)\n",
    "if \".DS_Store\" in fonts:\n",
    "    fonts.remove(\".DS_Store\")\n",
    "\n",
    "pexel_folder_dir = pexel_img_dir / \"pexel_images\"\n",
    "pexel_images = os.listdir(pexel_folder_dir)\n",
    "if \".DS_Store\" in pexel_images:\n",
    "    pexel_images.remove(\".DS_Store\")\n",
    "\n",
    "watermark_folder_dir = watermark_pexel_img_dir / \"watermark_pexel_images\"\n",
    "if watermark_folder_dir.exists() == False:\n",
    "    os.mkdir(watermark_folder_dir)\n",
    "\n",
    "curr_dir = None\n",
    "\n",
    "for im in tqdm(pexel_images):\n",
    "    img = Image.open(pexel_img_dir / im).convert(\"RGB\")\n",
    "    width, height, _ = np.array(img).shape\n",
    "    d = PIL.ImageDraw.Draw(img)\n",
    "    for i in range(np.random.randint(5, 50)):\n",
    "        font_file = np.random.choice(fonts)\n",
    "        fnt = PIL.ImageFont.truetype(str(font_dir / font_file), size=np.random.randint(20, 40))\n",
    "        fnt.size = np.random.randint(40, 125)\n",
    "        font_width = np.random.random() * width \n",
    "        font_height = np.random.random() * height\n",
    "        d.text(\n",
    "            (font_width, font_height), \n",
    "            ''.join([np.random.choice(list(string.digits + string.ascii_letters)) for x in range(20)]), \n",
    "            fill=(np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255)), \n",
    "            font=fnt,\n",
    "        )\n",
    "        img.save(watermark_pexel_img_dir / str(\"watermark_\" + im))\n",
    "        curr_dir = watermark_folder_img_dir / str(\"watermark_\" + im)\n",
    "\n",
    "curr_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make cycleGAN Model <a id=\"make-cycle-gan\"></a>\n",
    "\n",
    "Most of the code below is implemented from the [tensorflow tutorial on cycleGAN](https://www.tensorflow.org/tutorials/generative/cyclegan). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data <a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "train_normal = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  pexel_img_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  color_mode=\"rgb\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "train_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "test_normal = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  pexel_img_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  color_mode=\"rgb\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "train_watermark = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  watermark_pexel_img_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  color_mode=\"rgb\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "train_watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "test_watermark = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  watermark_pexel_img_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  color_mode=\"rgb\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def preprocess_image_train(image, label):\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_test(image, label):\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_normal = train_normal.map(\n",
    "  preprocess_image_train,\n",
    "  num_parallel_calls=AUTOTUNE\n",
    ").cache().shuffle(BUFFER_SIZE).batch(1)\n",
    "\n",
    "test_normal = test_normal.map(\n",
    "  preprocess_image_test, \n",
    "  num_parallel_calls=AUTOTUNE\n",
    ").cache().shuffle(BUFFER_SIZE).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_watermark = train_watermark.map(\n",
    "    preprocess_image_train, \n",
    "    num_parallel_calls=AUTOTUNE\n",
    ").cache().shuffle(BUFFER_SIZE).batch(1)\n",
    "\n",
    "test_watermark = test_watermark.map(\n",
    "    preprocess_image_test,\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ").cache().shuffle(BUFFER_SIZE).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_normal = next(iter(train_normal))\n",
    "sample_watermark = next(iter(train_watermark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models <a id=\"models\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_w = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_n = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_n = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_w = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions <a id=\"loss-functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss * 0.5\n",
    "\n",
    "\n",
    "def generator_loss(generated):\n",
    "    return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisers <a id=\"optimisers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_w_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_w_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints <a id=\"checkpoints\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_w,\n",
    "                           generator_f=generator_n,\n",
    "                           discriminator_x=discriminator_n,\n",
    "                           discriminator_y=discriminator_w,\n",
    "                           generator_g_optimizer=generator_w_optimizer,\n",
    "                           generator_f_optimizer=generator_n_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_n_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_w_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training <a id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "    fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(60, 46))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    for i in range(2):\n",
    "        ax[i].imshow(display_list[i] * 0.5 + 0.5) \n",
    "        ax[i].set_title(title[i])\n",
    "        ax[i].axis('off')\n",
    "    plt.savefig(fig_dir / \"curr_removal.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_n, real_w):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Y = W\n",
    "        # Generator W translates N -> W\n",
    "        # Generator N translates W -> N.\n",
    "        fake_w = generator_w(real_n, training=True)\n",
    "        cycled_n = generator_n(fake_w, training=True)\n",
    "        #\n",
    "        fake_n = generator_n(real_w, training=True)\n",
    "        cycled_w = generator_w(fake_n, training=True)\n",
    "        #\n",
    "        # same_n and same_w are used for identity loss.\n",
    "        same_n = generator_n(real_n, training=True)\n",
    "        same_w = generator_w(real_w, training=True)\n",
    "        #\n",
    "        disc_real_n = discriminator_n(real_n, training=True)\n",
    "        disc_real_w = discriminator_w(real_w, training=True)\n",
    "        #\n",
    "        disc_fake_n = discriminator_n(fake_n, training=True)\n",
    "        disc_fake_w = discriminator_w(fake_w, training=True)\n",
    "        #\n",
    "        # calculate the loss\n",
    "        gen_w_loss = generator_loss(disc_fake_w)\n",
    "        gen_n_loss = generator_loss(disc_fake_n)\n",
    "        #\n",
    "        total_cycle_loss = calc_cycle_loss(real_n, cycled_n) + calc_cycle_loss(real_w, cycled_w)\n",
    "        #\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_w_loss = gen_w_loss + total_cycle_loss + identity_loss(real_w, same_w)\n",
    "        total_gen_n_loss = gen_n_loss + total_cycle_loss + identity_loss(real_n, same_n)\n",
    "        #\n",
    "        disc_n_loss = discriminator_loss(disc_real_n, disc_fake_n)\n",
    "        disc_w_loss = discriminator_loss(disc_real_w, disc_fake_w)\n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_w_gradients = tape.gradient(total_gen_w_loss, generator_w.trainable_variables)\n",
    "    generator_n_gradients = tape.gradient(total_gen_n_loss, generator_n.trainable_variables)\n",
    "    #\n",
    "    discriminator_n_gradients = tape.gradient(disc_n_loss, discriminator_n.trainable_variables)\n",
    "    discriminator_w_gradients = tape.gradient(disc_w_loss, discriminator_w.trainable_variables)\n",
    "    #\n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_w_optimizer.apply_gradients(zip(generator_w_gradients, generator_w.trainable_variables))\n",
    "    #\n",
    "    generator_n_optimizer.apply_gradients(zip(generator_n_gradients, generator_n.trainable_variables))\n",
    "    #\n",
    "    discriminator_n_optimizer.apply_gradients(zip(discriminator_n_gradients,discriminator_n.trainable_variables))\n",
    "    #\n",
    "    discriminator_w_optimizer.apply_gradients(zip(discriminator_w_gradients,discriminator_w.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    start = time.time()\n",
    "    n = 0\n",
    "    for image_n, image_w in tf.data.Dataset.zip((train_normal, train_watermark)):\n",
    "        train_step(image_n[0], image_w[0])\n",
    "        if n % 10 == 0:\n",
    "              print ('.', end='')\n",
    "        n+=1\n",
    "    clear_output(wait=True)\n",
    "    # Using a consistent image (sample_watermark) so that the progress of the model\n",
    "    # is clearly visible.\n",
    "    generate_images(generator_n, sample_watermark[0])\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models <a id=\"save-models\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_w.save(model_dir / \"generator_w\")\n",
    "model_dir / \"generator_w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_n.save(model_dir / \"discriminator_n\")\n",
    "model_dir / \"discriminator_n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_n.save(model_dir / \"generator_n\")\n",
    "model_dir / \"generator_n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_w.save(model_dir / \"discriminator_w\")\n",
    "model_dir / \"discriminator_w\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
