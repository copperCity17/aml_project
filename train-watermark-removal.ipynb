{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n* [Introduction](#introduction)\n* [Imports](#imports)\n    - [Standard](#standard)\n    - [Matplotlib](#matplotlib)\n    - [Import OS and Get/Make Directories](#import-os)\n    - [Tensorflow & Keras](#tensorflow)\n    - [Other Tools](#other-tools)\n* [Generate Data](#generate-data)\n    - [Load Pexel Images](#load-pexel-images)\n        -[search_terms](#search-terms)\n        -[Scrape Image from Online](#scrape-images)\n    - [Add Watermarks to Images](#add-watermarks)\n* [Make cycleGAN Model](#make-cycle-gan)\n    - [Data](#data)\n    - [Models](#models)\n    - [Loss Functions](#loss-functions)\n    - [Optimisers](#optimisers)\n    - [Checkpoints](#checkpoints)\n    - [Training](#training)\n    - [Save Models](#save-models)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction  <a id=\"introduction\"></a>\n\nBefore training, the ensemble model is optimised by removing watermarks from images in both the train and test sets. To do this, a cycleGAN is developed, which is a combination of two GANs: hence there are two \"generators\" and two \"discriminators\".  The first generator, G_w, makes watermarked images from normal images. The second generator model, G_n, generates normal images from watermarked images. After training the cycleGAN, this generator model, G_n, is used to remove watermarks. Not only does this notebook develop the cycleGAN, but it also scrapes images from online for training the cycleGAN. "},{"metadata":{},"cell_type":"markdown","source":"# Imports <a id=\"imports\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Standard <a id=\"standard\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy  as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Matplotlib <a id=\"matplotlib\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"font.family\"] = \"Times New Roman\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import OS and Get/Make Directories <a id=\"import-os\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nfrom pathlib import Path\ncwd = os.path.abspath(os.getcwd())\ncwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_dir = Path(\"/\".join(cwd.split(\"/\"))) / \"figures\"\nif fig_dir.exists() == False:\n    os.mkdir(fig_dir)\n\nfig_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = Path(\"/\".join(cwd.split(\"/\"))) / \"data\"\nif data_dir.exists() == False:\n    os.mkdir(data_dir)\n\ndata_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pexel_img_dir = data_dir / \"pexel_img_dir\" \nif pexel_img_dir.exists() == False:\n    os.mkdir(pexel_img_dir)\n\npexel_img_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"watermark_pexel_img_dir = data_dir / \"watermark_pexel_img_dir\"\nif watermark_pexel_img_dir.exists() == False:\n    os.mkdir(watermark_pexel_img_dir)\n\nwatermark_pexel_img_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_dir = data_dir / \"fonts\"\nif font_dir.exists() == False:\n    os.mkdir(font_dir)\n\nfont_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dir = Path(cwd) / \"models\"\nif model_dir.exists() == False:\n    os.mkdir(model_dir)\n\nmodel_dir","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tensorflow & Keras <a id=\"tensorflow\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow_examples.models.pix2pix import pix2pix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other Tools <a id=\"other-tools\"></a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import PIL\nimport requests\nimport shutil\nimport string\nimport time\nimport cv2 \nfrom pexels_api import API \nfrom tqdm import tqdm\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nfrom IPython.display import clear_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Data <a id=\"generate-data\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Load Pexel Images <a id=\"load-pexel-images\"></a>\n\nThe first step towards training any model is to collect the training data. For this cycleGAN, stock-free images are scraped from Pexels with an [API Key](https://www.pexels.com/api/new/). "},{"metadata":{},"cell_type":"markdown","source":"### search_terms <a id=\"search-terms\"></a>\n\nBelow is the list of search terms used to search for stock-free images. While most search terms are related to food items because of the main objective of the Kaggle Project, some few terms are unrelated, like \"kitten\" and \"people\", to confuse and challenge the model during training."},{"metadata":{"trusted":true},"cell_type":"code","source":"search_terms = [\n  \"kitten\",\n  \"dog\",\n  \"food\",\n  \"cake\",\n  \"pasta\",\n  \"people\",\n  \"steak\",\n  \"cooked chicken\",\n  \"chicken wings\",\n  \"crab food\",\n  \"seafood\",\n  \"oyster\",\n  \"donut\",\n  \"burger\",\n  \"pizza\",\n  \"egg\",\n  \"avocado\",\n  \"bread\",\n  \"salad\",\n  \"sandwich\",\n  \"fries\",\n  \"butter\",\n  \"ham\",\n  \"sausage\",\n  \"bacon\",\n  \"dessert\",\n  \"rice\",\n  \"lasagna\",\n  \"green peas\",\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scrape Images from Online <a id=\"scrape-images\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"pexel_folder_dir = pexel_img_dir / \"pexel_images\"\nif pexel_folder_dir.exists() == False:\n    os.mkdir(pexel_folder_dir)\n\nPEXELS_API_KEY = \"563492ad6f91700001000001f8f1cdfad4e84affa3d8bb8ea5312020\"\n\napi = API(PEXELS_API_KEY)\n\nfor i, search_term in enumerate(tqdm(search_terms)):\n    print(\"search_term: \", search_term)\n    search = api.search(search_term, page=1, results_per_page=40)\n    photos = api.get_entries()\n    for j,photo in enumerate(photos):\n        img_url = photo.medium\n        filename = str(search_term + \"_\" + str(j) + \".jpg\")\n        r = requests.get(img_url, stream=True)\n        if r.status_code == 200:\n            with open(pexel_folder_dir / filename, 'wb') as f:\n                r.raw.decode_content = True\n                shutil.copyfileobj(r.raw, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add Watermarks to Images <a id=\"add-watermarks\"></a>\n\nThe next set in making the dataset for training is copy the collection of pexel images. With this copy, each image is watermarked. The result is that for every unmarked image, there is also an duplicate with watermarks, and vice-versa. While a cycleGAN does not need pairing a of images to train, it useful to still have this transition to stay organised and evaluate the performance of the cycleGAN. "},{"metadata":{},"cell_type":"markdown","source":"Some of the code in the block below is implemented from this [blog](https://rickwierenga.com/blog/machine%20learning/GanWatermark.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fonts = os.listdir(font_dir)\nif \".DS_Store\" in fonts:\n    fonts.remove(\".DS_Store\")\n\npexel_folder_dir = pexel_img_dir / \"pexel_images\"\npexel_images = os.listdir(pexel_folder_dir)\nif \".DS_Store\" in pexel_images:\n    pexel_images.remove(\".DS_Store\")\n\nwatermark_folder_dir = watermark_pexel_img_dir / \"watermark_pexel_images\"\nif watermark_folder_dir.exists() == False:\n    os.mkdir(watermark_folder_dir)\n\ncurr_dir = None\n\nfor im in tqdm(pexel_images):\n    img = Image.open(pexel_img_dir / im).convert(\"RGB\")\n    width, height, _ = np.array(img).shape\n    d = PIL.ImageDraw.Draw(img)\n    for i in range(np.random.randint(5, 50)):\n        font_file = np.random.choice(fonts)\n        fnt = PIL.ImageFont.truetype(str(font_dir / font_file), size=np.random.randint(20, 40))\n        fnt.size = np.random.randint(40, 125)\n        font_width = np.random.random() * width \n        font_height = np.random.random() * height\n        d.text(\n            (font_width, font_height), \n            ''.join([np.random.choice(list(string.digits + string.ascii_letters)) for x in range(20)]), \n            fill=(np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255)), \n            font=fnt,\n        )\n        img.save(watermark_pexel_img_dir / str(\"watermark_\" + im))\n        curr_dir = watermark_folder_img_dir / str(\"watermark_\" + im)\n\ncurr_dir","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make cycleGAN Model <a id=\"make-cycle-gan\"></a>\n\nMost of the code below is implemented from the [tensorflow tutorial on cycleGAN](https://www.tensorflow.org/tutorials/generative/cyclegan). "},{"metadata":{},"cell_type":"markdown","source":"## Data <a id=\"data\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nimg_height = 256\nimg_width = 256\n\ntrain_normal = tf.keras.preprocessing.image_dataset_from_directory(\n  pexel_img_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  color_mode=\"rgb\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)\n\ntrain_normal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nimg_height = 256\nimg_width = 256\n\ntest_normal = tf.keras.preprocessing.image_dataset_from_directory(\n  pexel_img_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  color_mode=\"rgb\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)\n\ntest_normal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nimg_height = 256\nimg_width = 256\n\ntrain_watermark = tf.keras.preprocessing.image_dataset_from_directory(\n  watermark_pexel_img_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  color_mode=\"rgb\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)\n\ntrain_watermark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nimg_height = 256\nimg_width = 256\n\ntest_watermark = tf.keras.preprocessing.image_dataset_from_directory(\n  watermark_pexel_img_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  color_mode=\"rgb\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)\n\ntest_watermark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(image):\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1\n    return image\n\ndef preprocess_image_train(image, label):\n    image = normalize(image)\n    return image\n\ndef preprocess_image_test(image, label):\n    image = normalize(image)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBUFFER_SIZE = 1000\n\ntrain_normal = train_normal.map(\n  preprocess_image_train,\n  num_parallel_calls=AUTOTUNE\n).cache().shuffle(BUFFER_SIZE).batch(1)\n\ntest_normal = test_normal.map(\n  preprocess_image_test, \n  num_parallel_calls=AUTOTUNE\n).cache().shuffle(BUFFER_SIZE).batch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBUFFER_SIZE = 1000\n\ntrain_watermark = train_watermark.map(\n    preprocess_image_train, \n    num_parallel_calls=AUTOTUNE\n).cache().shuffle(BUFFER_SIZE).batch(1)\n\ntest_watermark = test_watermark.map(\n    preprocess_image_test,\n    num_parallel_calls=AUTOTUNE\n).cache().shuffle(BUFFER_SIZE).batch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_normal = next(iter(train_normal))\nsample_watermark = next(iter(train_watermark))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models <a id=\"models\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\ngenerator_w = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\ngenerator_n = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n\ndiscriminator_n = pix2pix.discriminator(norm_type='instancenorm', target=False)\ndiscriminator_w = pix2pix.discriminator(norm_type='instancenorm', target=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss Functions <a id=\"loss-functions\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real, generated):\n    real_loss = loss_obj(tf.ones_like(real), real)\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss * 0.5\n\n\ndef generator_loss(generated):\n    return loss_obj(tf.ones_like(generated), generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LAMBDA = 10\n\ndef calc_cycle_loss(real_image, cycled_image):\n    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n    return LAMBDA * loss1\n\ndef identity_loss(real_image, same_image):\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    return LAMBDA * 0.5 * loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimisers <a id=\"optimisers\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator_w_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_w_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoints <a id=\"checkpoints\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\n\nckpt = tf.train.Checkpoint(generator_g=generator_w,\n                           generator_f=generator_n,\n                           discriminator_x=discriminator_n,\n                           discriminator_y=discriminator_w,\n                           generator_g_optimizer=generator_w_optimizer,\n                           generator_f_optimizer=generator_n_optimizer,\n                           discriminator_x_optimizer=discriminator_n_optimizer,\n                           discriminator_y_optimizer=discriminator_w_optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training <a id=\"training\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_images(model, test_input):\n    prediction = model(test_input)\n    plt.figure(figsize=(12, 12))\n    display_list = [test_input[0], prediction[0]]\n    title = ['Input Image', 'Predicted Image']\n    fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(60, 46))\n    plt.subplots_adjust(wspace=0.3)\n    for i in range(2):\n        ax[i].imshow(display_list[i] * 0.5 + 0.5) \n        ax[i].set_title(title[i])\n        ax[i].axis('off')\n    plt.savefig(fig_dir / \"curr_removal.png\", bbox_inches=\"tight\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(real_n, real_w):\n    # persistent is set to True because the tape is used more than\n    # once to calculate the gradients.\n    with tf.GradientTape(persistent=True) as tape:\n        # Y = W\n        # Generator W translates N -> W\n        # Generator N translates W -> N.\n        fake_w = generator_w(real_n, training=True)\n        cycled_n = generator_n(fake_w, training=True)\n        #\n        fake_n = generator_n(real_w, training=True)\n        cycled_w = generator_w(fake_n, training=True)\n        #\n        # same_n and same_w are used for identity loss.\n        same_n = generator_n(real_n, training=True)\n        same_w = generator_w(real_w, training=True)\n        #\n        disc_real_n = discriminator_n(real_n, training=True)\n        disc_real_w = discriminator_w(real_w, training=True)\n        #\n        disc_fake_n = discriminator_n(fake_n, training=True)\n        disc_fake_w = discriminator_w(fake_w, training=True)\n        #\n        # calculate the loss\n        gen_w_loss = generator_loss(disc_fake_w)\n        gen_n_loss = generator_loss(disc_fake_n)\n        #\n        total_cycle_loss = calc_cycle_loss(real_n, cycled_n) + calc_cycle_loss(real_w, cycled_w)\n        #\n        # Total generator loss = adversarial loss + cycle loss\n        total_gen_w_loss = gen_w_loss + total_cycle_loss + identity_loss(real_w, same_w)\n        total_gen_n_loss = gen_n_loss + total_cycle_loss + identity_loss(real_n, same_n)\n        #\n        disc_n_loss = discriminator_loss(disc_real_n, disc_fake_n)\n        disc_w_loss = discriminator_loss(disc_real_w, disc_fake_w)\n    # Calculate the gradients for generator and discriminator\n    generator_w_gradients = tape.gradient(total_gen_w_loss, generator_w.trainable_variables)\n    generator_n_gradients = tape.gradient(total_gen_n_loss, generator_n.trainable_variables)\n    #\n    discriminator_n_gradients = tape.gradient(disc_n_loss, discriminator_n.trainable_variables)\n    discriminator_w_gradients = tape.gradient(disc_w_loss, discriminator_w.trainable_variables)\n    #\n    # Apply the gradients to the optimizer\n    generator_w_optimizer.apply_gradients(zip(generator_w_gradients, generator_w.trainable_variables))\n    #\n    generator_n_optimizer.apply_gradients(zip(generator_n_gradients, generator_n.trainable_variables))\n    #\n    discriminator_n_optimizer.apply_gradients(zip(discriminator_n_gradients,discriminator_n.trainable_variables))\n    #\n    discriminator_w_optimizer.apply_gradients(zip(discriminator_w_gradients,discriminator_w.trainable_variables))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in tqdm(range(15, EPOCHS+15)):\n    start = time.time()\n    n = 0\n    for image_n, image_w in tf.data.Dataset.zip((train_normal, train_watermark)):\n        train_step(image_n[0], image_w[0])\n        if n % 10 == 0:\n              print ('.', end='')\n        n+=1\n    clear_output(wait=True)\n    # Using a consistent image (sample_watermark) so that the progress of the model\n    # is clearly visible.\n    generate_images(generator_n, sample_watermark[0])\n    if (epoch + 1) % 5 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))\n  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save Models <a id=\"save-models\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator_w.save(model_dir / \"generator_w\")\nmodel_dir / \"generator_w\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator_n.save(model_dir / \"discriminator_n\")\nmodel_dir / \"discriminator_n\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator_n.save(model_dir / \"generator_n\")\nmodel_dir / \"generator_n\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator_w.save(model_dir / \"discriminator_w\")\nmodel_dir / \"discriminator_w\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}